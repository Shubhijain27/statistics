{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "8YB-t4otKqwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Properties of F-Distribution\n",
        "\n",
        "Definition: The F-distribution depends on the degrees of freedom and is usually defined as the ratio of variances of two populations normally distributed and therefore it is also called as Variance Ratio Distribution.\n",
        "\n",
        "Skewness: The F-distribution is generally skewed to the right, meaning that it has a longer tail on the right side.\n",
        "\n",
        "Non-Normality: The F-distribution is not normally distributed, unlike the t-distribution. This means that the central limit theorem does not apply, and traditional normality assumptions may not hold.\n",
        "\n",
        "Dependence on degrees of freedom: The shape of the F-distribution depends on the values of the numerator (d1) and denominator (d2) degrees of freedom. As d1 increases, the distribution becomes more skewed, while as d2 increases, the distribution becomes more symmetric.\n",
        "\n",
        "Reciprocal Property: The F-distribution has a reciprocal property, where the area lying on the left-hand side of the distribution can be found by taking the reciprocal of F values corresponding to the right-hand side and interchanging the degrees of freedom.\n",
        "\n",
        "Moments: The k-th moment of an F(d1, d2) distribution exists and is finite only when 2k < d2. The moments are equal to the k-th power of the beta function.\n",
        "The variance is equal to [ 2 * v2^2 * ( v1 + v1 - 2 ) ] / [ v1 * ( v2 - 2 )^2 * ( v2 - 4 ) ] for v2 > 4.\n",
        "The mean of the distribution is equal to v2 / ( v2 - 2 ) for v2 > 2.\n"
      ],
      "metadata": {
        "id": "mvvrnEk_LiAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 . In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "B4UkbWyoLiEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An F-test is any statistical test used to compare the variances of two samples or the ratio of variances between multiple samples. The test statistic, random variable F, is used to determine if the tested data has an F-distribution under the true null hypothesis, and true customary assumptions about the error term (ε).[1] It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact \"F-tests\" mainly arise when the models have been fitted to the data using least squares"
      ],
      "metadata": {
        "id": "MvfoNA6xLiIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is used in various statistical tests, primarily for comparing the variances or proportions of different groups, and for evaluating the goodness-of-fit of statistical models. Some examples of tests that utilize the F-distribution include:\n",
        "\n",
        "ANOVA (Analysis of Variance): The F-test is used to compare the means of multiple groups to determine if there are significant differences between them. The F-statistic is the ratio of the variance between groups to the variance within groups, and it follows the F-distribution under the null hypothesis of equal means.\n",
        "F-test for equality of variances: This test is used to determine if the variances of two or more populations are equal. The F-statistic is the ratio of the larger variance to the smaller variance, and it follows the F-distribution under the null hypothesis of equal variances.\n",
        "Regression analysis: The F-statistic is used to evaluate the overall significance of a regression model, testing whether the independent variables collectively explain a significant amount of variance in the dependent variable.\n",
        "Comparison of multiple regression models: The F-test is used to compare the goodness-of-fit of multiple regression models, helping to determine which model best explains the data.\n",
        "Hypothesis testing for variance components: The F-distribution is used in tests for variance components, such as mixed-effects models, to evaluate the significance of random effects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The F-distribution is appropriate for these tests because it:\n",
        "\n",
        "Captures the ratio of variances: The F-distribution is well-suited for tests involving the ratio of variances, as it provides a natural framework for comparing the magnitude of different sources of variability.\n",
        "Has a well-defined null distribution: Under the null hypothesis, the F-statistic follows the F-distribution, allowing for precise calculations of critical values and p-values.\n",
        "Is robust to certain assumptions: While the F-test assumes normality and equal variances, it is relatively robust to minor deviations from these assumptions, making it a widely applicable statistical tool."
      ],
      "metadata": {
        "id": "QozindNfQHOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n"
      ],
      "metadata": {
        "id": "M8JfpvOEQHRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F-tests are used for other statistical tests of hypotheses, such as testing for differences in means in three or more groups, or in factorial layouts. These F-tests are generally not robust when there are violations of the assumption that each population follows the normal distribution, particularly for small alpha levels and unbalanced layouts. However, for large alpha levels (e.g., at least 0.05) and balanced layouts, the F-test is relatively robust, although (if the normality assumption does not hold) it suffers from a loss in comparative statistical power as compared with non-parametric counterparts.\n"
      ],
      "metadata": {
        "id": "a_FRIMnIRsoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normality: The populations from which the samples are drawn should have a normal distribution. The F-test is sensitive to deviations from normality, and if the distributions are not normal, the test can produce unpredictable results.\n",
        "\n",
        "Independence: The two populations should be independent of each other.\n",
        "\n",
        "Homogeneity of variance: The samples should be drawn randomly from the populations, and the variances of the two populations should be equal. This assumption is crucial, as the F-test is designed to detect differences in variances between the two populations.\n",
        "\n",
        "Additionally, some sources emphasize the importance of:\n",
        "\n",
        "Equal sample sizes: While not a strict assumption, equal or approximately equal sample sizes can improve the accuracy and robustness of the F-test.\n",
        "It’s essential to note that the F-test is sensitive to these assumptions, and deviations from them can impact the validity and reliability of the results. Therefore, it’s crucial to verify these assumptions before conducting the F-test, especially if the data does not meet these conditions.\n"
      ],
      "metadata": {
        "id": "kLQKJsY-QHT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "z2e-8ebtR4t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The t-test is a method that determines whether two populations are statistically different from each other, whereas ANOVA determines whether three or more populations are statistically different from each other. Both of them look at the difference in means and the spread of the distributions (i.e., variance) across groups; however, the ways that they determine the statistical significance are different.\n",
        "\n",
        "When are they used? These tests are performed when 1) the samples are independent of each other and 2) have (approximately) normal distributions or when the sample number is high (e.g., > 30 per group). More samples are better, but the tests can be performed with as little as 3 samples per condition.\n",
        "\n",
        "\n",
        "\n",
        "ANOVA Purpose: The primary purpose of Analysis of Variance (ANOVA) is to determine whether there are statistically significant differences between the means of three or more groups. ANOVA helps identify whether the variation in the data can be attributed to differences between groups or to random error.\n",
        "\n",
        "ANOVA vs T-test: ANOVA differs from a t-test in several key ways:\n",
        "\n",
        "Number of groups: ANOVA is designed for three or more groups, while a t-test is typically used for comparing the means of two groups.\n",
        "\n",
        "Assumptions: ANOVA assumes equal variances across groups, whereas t-tests assume equal variances or use non-parametric alternatives. ANOVA also assumes normality of the data, whereas t-tests are more robust to non-normality.\n",
        "\n",
        "Test statistic: ANOVA calculates an F-statistic, which measures the ratio of variance between groups to variance within groups. T-tests calculate a t-statistic, which measures the ratio of the mean difference to the standard error of the mean.\n",
        "\n",
        "Inference: ANOVA provides a global test of differences between multiple groups, whereas t-tests provide a pairwise comparison between two specific groups.\n",
        "\n",
        "Example scenarios: ANOVA is suitable for comparing average IQ scores across multiple countries, or evaluating the effectiveness of different treatments on a crop. T-tests are more suitable for comparing the average grades of students in two different classes or the average reaction times of participants in two different conditions."
      ],
      "metadata": {
        "id": "x811xYvLR4wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kKoZ83ZBTHg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two group"
      ],
      "metadata": {
        "id": "tfQs_2g3THCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way ANOVA: Used to test whether or not there is a statistically significant difference between the means of three or more groups when the groups can be split on one factor.\n",
        "\n",
        "Example: You randomly split up a class of 90 students into three groups of 30. Each group uses a different studying technique for one month to prepare for an exam. At the end of the month, all of the students take the same exam. You want to know whether or not the studying technique has an impact on exam scores so you conduct a one-way ANOVA to determine if there is a statistically significant difference between the mean scores of the three groups\n"
      ],
      "metadata": {
        "id": "Gupmh5DAR4y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple comparisons: When comparing more than two groups, using multiple t-tests can lead to an inflated Type I error rate (α) due to the increased number of tests. One-way ANOVA, on the other hand, controls for this issue by providing a single p-value that accounts for all comparisons simultaneously.\n",
        "Homogeneity of variances: One-way ANOVA assumes homogeneity of variances across groups, whereas multiple t-tests do not. If the variances are heterogeneous, ANOVA is more robust and can still provide a valid test, whereas multiple t-tests may be biased.\n",
        "Non-normality: While ANOVA is sensitive to non-normality, it can still be used with moderate deviations from normality. Multiple t-tests are more sensitive to non-normality and may require transformations or robust tests.\n",
        "Large sample sizes: With large sample sizes, ANOVA is more efficient than multiple t-tests, as it takes into account the overall variance structure across groups.Why use a one-way ANOVA over multiple t-tests:\n",
        "Simplified interpretation: ANOVA provides a single p-value and F-statistic, making it easier to interpret the results compared to multiple t-tests with separate p-values.\n",
        "Increased power: ANOVA can detect differences between groups more efficiently than multiple t-tests, especially when there are multiple comparisons.\n",
        "Reduced computational burden: ANOVA requires fewer calculations than multiple t-tests, making it more computationally efficient.\n",
        "When to use multiple t-tests instead of a one-way ANOVA:\n",
        "Small sample sizes: With small sample sizes, multiple t-tests may be more robust and provide more accurate results than ANOVA.\n",
        "Non-parametric data: If the data is non-parametric (e.g., ordinal or categorical), multiple non-parametric tests (e.g., Wilcoxon rank-sum tests) may be more appropriate than ANOVA.\n",
        "Specific hypotheses: If you have specific hypotheses about the differences between pairs of groups, multiple t-tests may be more suitable than ANOVA."
      ],
      "metadata": {
        "id": "0qIdb6qKULuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "HcPSzGGWaIqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA Variance Partitioning\n",
        "In Analysis of Variance (ANOVA), variance is partitioned into two components: between-group variance (also known as treatment variance) and within-group variance (also known as error variance). This partitioning is based on the law of total variance, which states that the total variance in a dataset can be decomposed into components attributable to different sources of variation.\n",
        "\n",
        "Between-group variance (ΣT): This component represents the variation between the means of different groups or treatments. It measures the differences between the group means, indicating whether the treatments have a significant effect on the response variable.\n",
        "\n",
        "Within-group variance (ΣW): This component represents the variation within each group, accounting for the random fluctuations or noise within each treatment. It measures the variability of individual observations within each group, regardless of the treatment.\n",
        "\n",
        "The partitioning of variance is achieved by calculating the sums of squares (SS) for both between-group (SSB) and within-group (SSE) components. The formulas are:\n",
        "\n",
        "Between-group sum of squares (SSB): SSB = Σ(ni * (mi - μ)^2), where ni is the sample size, mi is the mean of the ith group, and μ is the overall mean.\n",
        "Within-group sum of squares (SSE): SSE = ΣΣ(xi - mi)^2, where xi is an individual observation, mi is the mean of the ith group, and the summation is taken over all observations within each group.\n",
        "The total sum of squares (SST) is the sum of SSB and SSE:\n",
        "\n",
        "SST = SSB + SSE\n",
        "\n",
        "The F-statistic is then calculated as the ratio of the between-group variance (SSB) to the within-group variance (SSE), divided by the mean square within groups (MSW):\n",
        "\n",
        "F = (SSB / (k-1)) / (SSE / (N-k))\n",
        "\n",
        "where k is the number of groups (treatments), N is the total sample size, and (k-1) and (N-k) are the degrees of freedom for the between-group and within-group components, respectively.\n",
        "\n",
        "The F-statistic has an F-distribution with (k-1) and (N-k) degrees of freedom. The null hypothesis (H0) states that all group means are equal, while the alternative hypothesis (H1) states that at least one group mean differs from the others.\n",
        "\n",
        "If the calculated F-statistic is greater than the critical value from the F-distribution, or if the p-value is below a predetermined significance level (e.g., 0.05), the null hypothesis is rejected, indicating that the differences between group means are statistically significant.\n",
        "\n",
        "In summary, the partitioning of variance in ANOVA into between-group and within-group components enables the calculation of the F-statistic, which tests the null hypothesis that all group means are equal. The F-statistic is sensitive to the differences between group means (between-group variance) relative to the variability within each group (within-group variance"
      ],
      "metadata": {
        "id": "IPl7U1BLaIwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "x-FwPIYxaIzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA (Analysis of Variance) is a statistical technique used to compare means of multiple groups. Both classical (frequentist) and Bayesian approaches can be applied to ANOVA, but they differ significantly in their treatment of uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "Uncertainty Handling\n",
        "\n",
        "Frequentist: Views uncertainty as a property of the population, focusing on long-run frequencies. The null hypothesis is tested using p-values, which represent the probability of observing the data (or more extreme) assuming the null hypothesis is true. The emphasis is on rejecting or failing to reject the null hypothesis.\n",
        "\n",
        "Bayesian: Treats uncertainty as a reflection of the analyst’s degree of belief, updating probabilities based on new data. Bayesians assign prior probabilities to hypotheses and update them using likelihood functions. The focus is on estimating the posterior distribution of parameters and making probabilistic statements about hypotheses.\n",
        "Parameter Estimation\n",
        "\n",
        "Frequentist: Estimates population parameters (e.g., means, variances) using sample statistics (e.g., sample means, sample variances). The goal is to obtain a single, fixed estimate of the population parameter.\n",
        "Bayesian: Estimates posterior distributions of parameters, which reflect the uncertainty in the estimates. The posterior distribution combines the prior distribution and the likelihood function, providing a range of possible values for the parameter.\n",
        "\n",
        "Hypothesis Testing\n",
        "Frequentist: Tests hypotheses using null and alternative hypotheses, with a focus on rejecting or failing to reject the null hypothesis. The p-value is used to determine the significance of the test.\n",
        "\n",
        "Bayesian: Formulates hypotheses as statements about the probability of the data given the hypothesis. The Bayesian approach estimates the posterior probability of the hypothesis, allowing for a more nuanced interpretation of the results. Bayesians can also use decision-theoretic approaches to choose the most informative hypothesis.\n",
        "\n",
        "Key Differences\n",
        "Probability interpretation: Frequentists view probability as a long-run frequency, while Bayesians treat probability as a degree of belief.\n",
        "\n",
        "Parameter estimation: Frequentists aim for a single, fixed estimate, whereas Bayesians provide a posterior distribution reflecting uncertainty.\n",
        "\n",
        "Hypothesis testing: Frequentists focus on rejecting or failing to reject null hypotheses, while Bayesians estimate posterior probabilities of hypotheses and can make probabilistic statements about them.\n",
        "\n",
        "Uncertainty quantification: Bayesians provide a comprehensive quantification of uncertainty through posterior distributions, whereas frequentists rely on p-values and confidence intervals."
      ],
      "metadata": {
        "id": "HL61utNgbTIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "Profession A: [48, 52, 55, 60, 62]\n",
        "Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison"
      ],
      "metadata": {
        "id": "hvwEOjnZbiri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Data for Profession A and Profession B\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate variances\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "# Calculate F-statistic\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "# Calculate degrees of freedom\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = f.sf(F_statistic, df_A, df_B)\n",
        "\n",
        "print(\"F-statistic:\", F_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. Variances are not equal.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. Variances are equal.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR5g-K_W3jq9",
        "outputId": "364e8591-aa50-4cea-dbf6-29e1cfb8c046"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.24652429950266966\n",
            "Fail to reject the null hypothesis. Variances are equal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "\n",
        "Based on the F-test, we fail to reject the null hypothesis that the variances of the two professions' incomes are equal (p-value = 0.283, α = 0.05). This suggests that the variances are likely equal.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- F-statistic: Measures the ratio of variances between the two groups.\n",
        "- p-value: Probability of observing the test statistic (or more extreme) assuming equal variances.\n",
        "- α (significance level): Threshold for rejecting the null hypothesis (usually 0.05).\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "- Normality: Data should be normally distributed (not severely skewed).\n",
        "- Independence: Observations should be independent.\n",
        "\n",
        "Note: The F-test assumes equal sample sizes and normality. For unequal sample sizes or non-normal data, consider alternative tests (e.g., Levene's test, Brown-Forsythe test)."
      ],
      "metadata": {
        "id": "WUDTbL7q39cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        " Region A: [160, 162, 165, 158, 164]\n",
        " Region B: [172, 175, 170, 168, 174]\n",
        " Region C: [180, 182, 179, 185, 183]\n",
        "\n",
        " Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "\n",
        " Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value"
      ],
      "metadata": {
        "id": "kf55qQlMbvsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pCsxZNp4bvpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data\n",
        "region_A = np.array([160, 162, 165, 158, 164])\n",
        "region_B = np.array([172, 175, 170, 168, 174])\n",
        "region_C = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# One-way ANOVA\n",
        "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "print(\"F-statistic: \", F_statistic)\n",
        "print(\"p-value: \", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There are statistically significant differences in average heights between regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. No statistically significant differences in average heights between regions.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_32LNqV4bTpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5db58c-f247-4be8-f470-ecf7c7b3d41b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic:  67.87330316742101\n",
            "p-value:  2.870664187937026e-07\n",
            "Reject the null hypothesis. There are statistically significant differences in average heights between regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "1. Import necessary libraries.\n",
        "2. Define the data for each region.\n",
        "3. Perform one-way ANOVA using stats.f_oneway.\n",
        "4. Print F-statistic and p-value.\n",
        "5. Interpret results based on chosen significance level (alpha).\n",
        "\n",
        "Interpretation Guidelines:\n",
        "\n",
        "- If p-value < alpha, reject the null hypothesis, indicating statistically significant differences between group means.\n",
        "- If p-value >= alpha, fail to reject the null hypothesis, suggesting no statistically significant differences between group means.\n",
        "\n",
        "\n",
        "This result indicates that there are statistically significant differences in average heights between the three regions"
      ],
      "metadata": {
        "id": "H8Gt9Zma4WEd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTJr8l5o4Un4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}